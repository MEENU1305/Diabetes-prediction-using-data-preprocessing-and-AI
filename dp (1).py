# -*- coding: utf-8 -*-
"""DP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PZc4B3AdvqoEoeVrXOaXXr2f7P0RLk9x

importing the dependencies
"""

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
# !pip install missingno
import missingno as msno
from datetime import date
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler


import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Adjustment of visibility of Datafreames
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.float_format', lambda x: '%.3f' % x)
pd.set_option('display.width', 500)

df = pd.read_csv("/content/diabetes.csv")
df.head()

print("**************** Shape ******")
print(df.shape)
print("##################### Types #####################")
print(df.dtypes)
print("##################### NA #####################")
print(df.isnull().sum())
print("##################### Quantiles #####################")
print(df.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)

df.describe().T

df['Outcome'].value_counts()

df.groupby('Outcome').mean()

def grab_col_names(dataframe, cat_th=10, car_th=20):
    # cat_cols, cat_but_car
    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == "O"]
    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and
                   dataframe[col].dtypes != "O"]
    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and
                   dataframe[col].dtypes == "O"]
    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    # num_cols
    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != "O"]
    num_cols = [col for col in num_cols if col not in num_but_cat]

    # report
    print(f"Observations: {dataframe.shape[0]}")
    print(f"Variables: {dataframe.shape[1]}")
    print(f'cat_cols: {len(cat_cols)}')  # the number of categorical variables

def grab_col_names(dataframe, cat_th=10, car_th=20):
    # cat_cols, cat_but_car
    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == "O"]
    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and
                   dataframe[col].dtypes != "O"]
    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and
                   dataframe[col].dtypes == "O"]
    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    # num_cols
    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != "O"]
    num_cols = [col for col in num_cols if col not in num_but_cat]

    # report
    print(f"Observations: {dataframe.shape[0]}")
    print(f"Variables: {dataframe.shape[1]}")
    print(f'cat_cols: {len(cat_cols)}')  # the number of categorical variables
    print(f'num_cols: {len(num_cols)}')  # the number of numerical variables
    print(f'cat_but_car: {len(cat_but_car)}')  # the number of cardinal variables
    print(f'num_but_cat: {len(num_but_cat)}')  # the number of categorical variables that looks numerical
    return cat_cols, num_cols, cat_but_car


cat_cols, num_cols, cat_but_car = grab_col_names(df)

print("Categorical columns:",cat_cols)
print("Numerical columns:", num_cols)
print("Cardinal columns:", cat_but_car)

# Target variable analysis
# The average of the numerical variables according to the target variable
df.groupby(cat_cols)[num_cols].mean()

# The average of the target variable according to the categorical variables
df[cat_cols].mean()

print(df.Outcome.value_counts())
print("-------------------------------")
print(df[cat_cols].mean())
print("-------------------------------")
print("Ratio: ")
(df.Outcome.value_counts() / len(df)) *100

"""Analyse outliers and missing values¬∂

There are 2 points that call attention to the first look:

"Insulin" has a high standard deviation, the quartile values are large, and the outlier is clear.

"SkinThickness" quartile distribution is uneven
"""

df.describe().T

# Boxplot
sns.boxplot(x=df["Insulin"])
plt.show()

sns.boxplot(x=df["SkinThickness"])
plt.show()

def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):
    quartile1 = dataframe[col_name].quantile(q1)
    quartile3 = dataframe[col_name].quantile(q3)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

for i in df.columns:
    print("Thresholds of {} : ({:.2f}, {:.2f})".format(i, *outlier_thresholds(df,i)))

# Observe the outliers, it can be functioned as well, but we don't need here
low, up = outlier_thresholds(df, "Pregnancies")
df[((df["Pregnancies"] < low) | (df["Pregnancies"] > up))]

"""Missing Values"""

df.isnull().sum()

df.head(10)

num_cols_miss = [i for i in num_cols if i != "Pregnancies"]
for i in num_cols_miss:
    df[i] = df.apply(lambda x: np.nan if x[i] == 0 else x[i], axis=1)

# df after adding NaN
df.head(10)

df.isnull().sum()

# the number of non-missing data
msno.bar(df)
plt.show()

msno.matrix(df)
plt.show()

"""
heatmap can be used rather than try to observe with eyes on matrix map
this heatmap shows the correlation of missing values on variables
"""
msno.heatmap(df)
plt.show()

def missing_values_table(dataframe, na_name=False):
    # only take missing columns
    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]

    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)  # number of missing value
    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False) # ratio
    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])  # make table
    print(missing_df, end="\n")

    if na_name:
        return na_columns

missing_values_table(df)

# if column names wants to be stored, na_name argument can be made True
na_cols = missing_values_table(df, na_name=True)
na_cols

# We created na_cols above, it can be taken with similar way in function:
# na_columns = [i for i in df.columns if df[i].isnull().sum() > 0]
def missing_vs_target(dataframe, target, na_columns):
    temp_df = dataframe.copy()

    for col in na_columns:
        temp_df[col + '_NA_FLAG'] = np.where(temp_df[col].isnull(), 1, 0)  # eksiklik varsa 1, yoksa 0 yaz

    # [t√ºm satƒ±rlarƒ± getir, s√ºtunlarda i√ßinde NA olanlarƒ± getir]
    na_flags = temp_df.loc[:, temp_df.columns.str.contains("_NA_")].columns

    for col in na_flags:
        print(pd.DataFrame({"TARGET_MEAN": temp_df.groupby(col)[target].mean(),
                            "Count": temp_df.groupby(col)[target].count()}), end="\n\n\n")


missing_vs_target(df, "Outcome", na_cols)

"""relation between target variable(Outcome) and another variables"""

# We created na_cols above, it can be taken with similar way in function:
# na_columns = [i for i in df.columns if df[i].isnull().sum() > 0]
def missing_vs_target(dataframe, target, na_columns):
    temp_df = dataframe.copy()

    for col in na_columns:
        temp_df[col + '_NA_FLAG'] = np.where(temp_df[col].isnull(), 1, 0)  # eksiklik varsa 1, yoksa 0 yaz

    # [t√ºm satƒ±rlarƒ± getir, s√ºtunlarda i√ßinde NA olanlarƒ± getir]
    na_flags = temp_df.loc[:, temp_df.columns.str.contains("_NA_")].columns

    for col in na_flags:
        print(pd.DataFrame({"TARGET_MEAN": temp_df.groupby(col)[target].mean(),
                            "Count": temp_df.groupby(col)[target].count()}), end="\n\n\n")


missing_vs_target(df, "Outcome", na_cols)

df_cor = pd.DataFrame([df["Outcome"].corr(df[i]) for i in num_cols], index=num_cols, columns=["Correlation"])
df_cor

df_new = df.copy() # copy dataset to see effect without damage the main dataset
df_new.shape

df_new.dropna(inplace=True)
df_new.shape

df_fill = df.apply(lambda x: x.fillna(x.median()) if x.dtype != "O" else x, axis=0)
df_fill.head(10)

df_ml = df.copy()

# standardization
scaler = MinMaxScaler()
df_ml = pd.DataFrame(scaler.fit_transform(df_ml), columns=df_ml.columns)
df_ml.head()

# fill with KNN
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
df_ml = pd.DataFrame(imputer.fit_transform(df_ml), columns=df_ml.columns)
df_ml.head()

df_ml = pd.DataFrame(scaler.inverse_transform(df_ml), columns=df_ml.columns)
df_ml.head()

# our first dataset
df.head()

missing_values_table(df)

na_cols = missing_values_table(df, na_name=True) # columns that includes missing values
n_miss = df[na_cols].isnull().sum() # number of missing values on variables

# 100 as a threshold, it is open to comment
na_cols_ml = [i for i in n_miss.index if n_miss[i] < 100]
na_cols_med = [i for i in n_miss.index if n_miss[i] > 100]
print("Columns that will be applied ML model:", na_cols_ml)
print("Columns that will be filled with median:", na_cols_med)

# check before process
df.head(10)

# for the number of missing value is less than 100
df[na_cols_med] = df[na_cols_med].apply(lambda x: x.fillna(x.median()) if x.dtype != "O" else x, axis=0)
df.head(10)

""", "SkinThickness" and "Insulin" have been filled with their median values and the variables that have slight number of missing values will be filled with ML model"""

# standardization
scaler = MinMaxScaler()

# take only needed columns
df[na_cols_ml] = pd.DataFrame(scaler.fit_transform(df[na_cols_ml]), columns=df[na_cols_ml].columns)
print(df[na_cols_ml].head())

# fill with KNN
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
df[na_cols_ml] = pd.DataFrame(imputer.fit_transform(df[na_cols_ml]), columns=df[na_cols_ml].columns)
print(df[na_cols_ml].head())

# from standardized to non-standardized
df[na_cols_ml] = pd.DataFrame(scaler.inverse_transform(df[na_cols_ml]), columns=df[na_cols_ml].columns)
print(df[na_cols_ml].head())

df.head(10)

df.isnull().sum()

df_ex = pd.DataFrame({"age": [17, 35, 47],
                     "pregnancy": [5, 2, 3]})
df_ex

"""age as sibgle feature doesnt act as outlier but age and pregnacy are comapred or taken together than they act as outliers.the values have meanings separately, but together, a person 17 years old cannot be pregnant 5 times. This is an outlier row. LOF helps us to find these kinds of values and fix these."""

# LOF
clf = LocalOutlierFactor(n_neighbors=20)
clf.fit_predict(df)  # returns LOF scores
df_scores = clf.negative_outlier_factor_ # keep scores to observe (negative)
# df_scores = -df_scores # for changing to pozitive but we will use as negative
print(df_scores[0:5])
print(np.sort(df_scores)[0:5])

# elbow method
scores = pd.DataFrame(np.sort(df_scores))
scores.plot(stacked=True, xlim=[0, 20], style='.-')
plt.show()

th = np.sort(df_scores)[3]  # set any lower scores than that as outlier
df[df_scores < th] # check outliers

df.describe([0.01, 0.05, 0.25, 0.50, 0.75, 0.90, 0.99]).T

print("Before delete outliers:", df.shape)
print(df[df_scores < th].index) # indexes of outliers, just for observation
df.drop(axis=0, labels=df[df_scores < th].index, inplace=True)
print("After delete outliers:", df.shape)

def replace_with_thresholds(dataframe, variable):
    low_limit, up_limit = outlier_thresholds(dataframe, variable)
    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit
    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit

for col in num_cols:
    replace_with_thresholds(df, col)

# after re-assignment
df.describe().T

# pregrancy cannot be float, it occurs due to calculation of IQR
df["Pregnancies"] = df["Pregnancies"].apply(lambda x: int(x))

""". Feature Extraction¬∂
When a data set is prepared, not only existing variables are tried to be edited, but also new, meaningful variables have to be created. New columns sometimes can be created with mathematical operations, sometimes named a numerical value to categorical, or categorical values' ranges, etc. This process is known as Feature Engineering, and this is one of the critical parts of data preparation.
"""

# create categorical columns from numerical columns

# if bins are 0, 3, 6 => 0 values become NaN due to bins
df["NumOfPreg"] = pd.cut(df["Pregnancies"], bins=[-1, 3, 6, df["Pregnancies"].max()], labels=["Normal", "Above Normal","Extreme"])
df["AgeGroup"] = pd.cut(df["Age"], bins=[18, 25, 40, df["Age"].max()], labels=["Young", "Mature", "Old"])
df["GlucoseGroup"] = pd.qcut(df["Glucose"], 3, labels=["Low", "Medium", "High"])
df["Patient"] = np.where(df["Outcome"] == 1, "Yes", "No")

# example of mathematical expression

"""Assume there is a variable named "BMIns", and it can be found with the multiplication of BMI and Insuline.
Create and add it to data frame"""

df["BMIns"] = df["BMI"]*df["Insulin"] # numerical
df["BMInsGroup"] = pd.qcut(df["BMIns"], 3, labels=["Low", "Medium", "High"]) # categorical

df.head()

"""Encoding & Scaling"""

df_enc = df.copy()

le = LabelEncoder()
le.fit_transform(df_enc["Patient"])[0:5]

# let's say we forgot which 0 and which 1, inverse_transform is used to detect this
le.inverse_transform([0, 1])

# detect variables that have 2 unique numbers for Binary Encoding
binary_cols = [col for col in df.columns if df[col].dtype not in [int, float]
               and df[col].nunique() == 2]
binary_cols

"""If a variable is converted like this to numeric values and sent to algorithms, the algorithm defines this variable as a variable ranging from an interval. In other words, this transformation will confuse the algorithms. The reason for its corruption is here, the categorical variable has turned into numerical values, and a categorical variable, which is normally a nominal scale type, is suddenly distorted by a transformation process and is subjected to a sorting process as if there is a difference between them. The characteristic of the nominal scale type is that the distance between classes is equal, but this equality is disturbed after this transformation.

This situation can cause biases and problems when we convert categorical variables to numeric values and confuse algorithms. In this case, we need to do one-hot encoding.

To fix this problem, one-hot conversion is done, but doing this also creates a trap named as "dummy variable trap."

If we apply a transformation to the variables in the data set, if the new variables can be created over each other, this is a dummy variable trap. In other words, if there is a variable that represents another variable, this situation is called a dummy variable trap.
"""

df.head()

df_new = pd.get_dummies(df, columns=["Patient"])
df_new.head()

df_new = pd.get_dummies(df, columns=["Patient"], drop_first=True, prefix=["Sick"])
df_new.head()

cat_cols, num_cols, cat_but_car = grab_col_names(df)
cat_cols

cat_cols_final = [i for i in cat_cols if i not in ["Patient", "Outcome"]]
df = pd.get_dummies(df, columns=cat_cols_final, drop_first=True)
df.head()

"""Scaling¬∂
It is a variable scaling operation. The aim is to eliminate the measurement difference between the variables and to try to ensure that the models to be used approach the variables under equal conditions. Tree-based methods are not affected by outlier, standardization. In the general trend, we may prefer to scale features.

The variance structure and information structure within the variable itself are not deteriorated, but are set to a certain standard. For example, let's say a dataset has a value of 10 and is ranked 80th when ordered from smallest to largest. When this variable is standardized, the value of 10 will be something like between 1-2 or 0 to 1, but when the data set is again ordered from smallest to largest, this value will still correspond to the 80th value. Therefore, when a variable is standardized, the value of the variable will change, it will be put into a certain format, but the spread and the essence of the distribution information will not change. The values of the variable such as mean, standard deviation and variance will change,as well, but the spread, distribution and current state of the information it carries within the variable will not change.

Scaling¬∂
It is a variable scaling operation. The aim is to eliminate the measurement difference between the variables and to try to ensure that the models to be used approach the variables under equal conditions. Tree-based methods are not affected by outlier, standardization. In the general trend, we may prefer to scale features.

The variance structure and information structure within the variable itself are not deteriorated, but are set to a certain standard. For example, let's say a dataset has a value of 10 and is ranked 80th when ordered from smallest to largest. When this variable is standardized, the value of 10 will be something like between 1-2 or 0 to 1, but when the data set is again ordered from smallest to largest, this value will still correspond to the 80th value. Therefore, when a variable is standardized, the value of the variable will change, it will be put into a certain format, but the spread and the essence of the distribution information will not change. The values of the variable such as mean, standard deviation and variance will change,as well, but the spread, distribution and current state of the information it carries within the variable will not change.
"""

df_scale = df.copy()

# standart scaler
ss = StandardScaler()
df_scale["Age_standard_scaler"] = ss.fit_transform(df_scale[["Age"]])
df_scale.head()

# robust scaler
rs = RobustScaler()
df_scale["Age_robuts_scaler"] = rs.fit_transform(df_scale[["Age"]])

# min-max scaler
# The range can be given with the feature_range=() argument
mms = MinMaxScaler() # default range from 0 to 1
df_scale["Age_min_max_scaler"] = mms.fit_transform(df_scale[["Age"]])

df_scale.head()

# Assuming your cleaned DataFrame is named `df_cleaned`
df_scale.to_csv('cleaned_diabetes_data.csv', index=False)

rs = RobustScaler()
for i in num_cols:
      df[i] = rs.fit_transform(df[[i]])
df.head()

from google.colab import files
files.download('cleaned_diabetes_data.csv')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score

# Drop columns that are not needed
columns_to_drop = ['Patient']
df_model = df.drop(columns=columns_to_drop)

# Keep only the first 8 features + target
# Get feature column names (excluding 'Outcome')
feature_columns = df_model.drop(columns=['Outcome']).columns[:8]
df_model = df_model[feature_columns.tolist() + ['Outcome']]

# Define features and target
X = df_model.drop(columns=['Outcome'])
y = df_model['Outcome']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Dictionary to store models and their evaluation
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC(probability=True),
    "KNN": KNeighborsClassifier()

}

results = {}

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    results[name] = {
        "train_accuracy": accuracy_score(y_train, y_pred_train),
        "test_accuracy": accuracy_score(y_test, y_pred_test),
        "classification_report": classification_report(y_test, y_pred_test, output_dict=True),
        "model": model
    }

results

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)  # Train the model before using it

def predict_diabetes_rf(input_data):
    input_array = np.array(input_data).reshape(1, -1)
    prediction = rf_model.predict(input_array)[0]
    probability = rf_model.predict_proba(input_array)[0][1]
    return int(prediction), float(probability)

predict_diabetes_rf([4,129,86,20,145,35.1,0.231,23])

import pandas as pd
import os
import numpy as np
from datetime import datetime
from IPython.display import display, HTML

# Sample input (replace this with your actual input)
sample_input = [1	,81,	72	,18	,105	,26.6,	0.283,	24]  # 8 features

# Define the feature names
feature_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']

# Create DataFrame for prediction
X_input = pd.DataFrame([sample_input], columns=feature_names)

# Make prediction
prediction = rf_model.predict(X_input)[0]
probability = rf_model.predict_proba(X_input)[0][1]

# Build record to log
record = sample_input + [int(prediction), float(probability), datetime.now().strftime("%Y-%m-%d %H:%M:%S")]
columns = feature_names + ['Prediction', 'Probability', 'Timestamp']
df_record = pd.DataFrame([record], columns=columns)

# File path to save log
csv_path = "/content/diabetes_predictions_logfile.csv"

# Display prediction
if prediction == 0:
    print("üü¢ The person is **not diabetic**")
else:
    print("üî¥ The person is **diabetic**")

# Append or create the CSV file
if os.path.exists(csv_path):
    df_existing = pd.read_csv(csv_path)

    # Convert to same types to ensure correct comparison
    for col in feature_names:
        df_existing[col] = pd.to_numeric(df_existing[col], errors='coerce')

    duplicate = ((df_existing[feature_names] == X_input.iloc[0]).all(axis=1)).any()
else:
    duplicate = False

if duplicate:
    print("‚ö†Ô∏è Same input already exists in log ‚Äî not appending again.")
else:
    if not os.path.exists(csv_path):
        df_record.to_csv(csv_path, index=False)
    else:
        df_record.to_csv(csv_path, mode='a', header=False, index=False)
    print("üìÑ Entry saved to:", csv_path)

# Display the last 10 records in the log
df_log = pd.read_csv(csv_path)
display(HTML("<h4>üìã Full Prediction Log</h4>"))
display(df_log.tail(10))